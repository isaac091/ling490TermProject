{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H8YoANNikUU0","executionInfo":{"status":"ok","timestamp":1669882923087,"user_tz":360,"elapsed":15013,"user":{"displayName":"Isaac Schifferer","userId":"17126994690334067036"}},"outputId":"98243333-2614-46ce-8442-1957aa226049"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install -q -U \"tensorflow-text==2.8.*\"\n","!pip install -q tf-models-official==2.7.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uchYNR1nRSsb","executionInfo":{"status":"ok","timestamp":1669882991954,"user_tz":360,"elapsed":68871,"user":{"displayName":"Isaac Schifferer","userId":"17126994690334067036"}},"outputId":"71e903cf-c2bd-4237-a032-488ce52592d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.9 MB 8.3 MB/s \n","\u001b[K     |████████████████████████████████| 498.0 MB 13 kB/s \n","\u001b[K     |████████████████████████████████| 5.8 MB 86.5 MB/s \n","\u001b[K     |████████████████████████████████| 462 kB 86.8 MB/s \n","\u001b[K     |████████████████████████████████| 1.4 MB 83.7 MB/s \n","\u001b[K     |████████████████████████████████| 1.8 MB 8.5 MB/s \n","\u001b[K     |████████████████████████████████| 118 kB 84.8 MB/s \n","\u001b[K     |████████████████████████████████| 352 kB 83.3 MB/s \n","\u001b[K     |████████████████████████████████| 238 kB 87.4 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 68.1 MB/s \n","\u001b[K     |████████████████████████████████| 1.3 MB 61.0 MB/s \n","\u001b[K     |████████████████████████████████| 43 kB 2.4 MB/s \n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","import shutil\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from official.nlp import optimization  # to create AdamW optimizer\n","import tensorflow_addons as tfa\n","\n","tf.get_logger().setLevel('ERROR')"],"metadata":{"id":"nQSrkTtiRTUW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z9CpVP4IRfoy","executionInfo":{"status":"ok","timestamp":1669882996427,"user_tz":360,"elapsed":12,"user":{"displayName":"Isaac Schifferer","userId":"17126994690334067036"}},"outputId":"5746bb56-205b-4181-afa3-c6f9f13ee25c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","source":["# Create train set\n","emotions = [\"anger\", \"anticipation\", \"fear\", \"joy\", \"sadness\"]\n","data_file_train = \"/content/drive/MyDrive/ling490/TermProject/data/small.csv\"\n","train_lang = \"en\"\n","\n","for e in emotions: os.makedirs(f\"/content/train/{train_lang}/{e}\")\n","\n","df = pd.read_csv(data_file_train)\n","sentences = [[],[],[],[],[]]\n","for idx,row in df.loc[df['language'] == train_lang].iterrows():\n","  sentences[emotions.index(row[1])].append(row[0])\n","\n","for i,e in enumerate(sentences):\n","  train_sents = e[:int(len(e)*.8)]\n","  for j,sent in enumerate(train_sents):\n","    with open(f\"/content/train/{train_lang}/{emotions[i]}/{j}.txt\", 'w') as f: f.write(sent)"],"metadata":{"id":"OmVidtHnY-2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create test set for large languages from file 1\n","emotions = [\"anger\", \"anticipation\", \"fear\", \"joy\", \"sadness\"]\n","data_file_test = \"/content/drive/MyDrive/ling490/TermProject/data/small.csv\"\n","test_langs = [\"en\", \"tl\", \"zh\", \"pt\"]\n","\n","for test_lang in test_langs:\n","  for e in emotions: os.makedirs(f\"/content/test/{test_lang}/{e}\")\n","\n","  df = pd.read_csv(data_file_test)\n","  sentences = [[],[],[],[],[]]\n","  for idx,row in df.loc[df['language'] == test_lang].iterrows():\n","    sentences[emotions.index(row[1])].append(row[0])\n","\n","  for i,sents in enumerate(sentences):\n","    test_sents = sents[int(len(sents)*.8):]\n","    for j,sent in enumerate(test_sents):\n","      with open(f\"/content/test/{test_lang}/{emotions[i]}/{j}.txt\", 'w') as f: f.write(sent)"],"metadata":{"id":"xXVPJ8I-TAFn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create test set for large languages from file 2\n","emotions = [\"anger\", \"anticipation\", \"fear\", \"joy\", \"sadness\"]\n","data_file_test = \"/content/drive/MyDrive/ling490/TermProject/data/low-resource.csv\"\n","test_langs = [\"de\", \"fr\", \"id\", \"th\", \"vi\"]\n","\n","for test_lang in test_langs:\n","  for e in emotions: os.makedirs(f\"/content/test/{test_lang}/{e}\")\n","\n","  df = pd.read_csv(data_file_test)\n","  sentences = [[],[],[],[],[]]\n","  for idx,row in df.loc[df['language'] == test_lang].iterrows():\n","    sentences[emotions.index(row[1])].append(row[0])\n","\n","  for i,sents in enumerate(sentences):\n","    test_sents = sents[int(len(sents)*.8):]\n","    for j,sent in enumerate(test_sents):\n","      with open(f\"/content/test/{test_lang}/{emotions[i]}/{j}.txt\", 'w') as f: f.write(sent)"],"metadata":{"id":"90T48-80y_N5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create test set for small languages from file 2\n","emotions = [\"anger\", \"anticipation\", \"fear\", \"joy\", \"sadness\"]\n","data_file_test = \"/content/drive/MyDrive/ling490/TermProject/data/low-resource.csv\"\n","test_langs = [\"km\", \"bn\", \"my\"]\n","\n","for test_lang in test_langs:\n","  for e in emotions: os.makedirs(f\"/content/test/{test_lang}/{e}\")\n","\n","  df = pd.read_csv(data_file_test)\n","  sentences = [[],[],[],[],[]]\n","  for idx,row in df.loc[df['language'] == test_lang].iterrows():\n","    sentences[emotions.index(row[1])].append(row[0])\n","\n","  for i,sents in enumerate(sentences):\n","    for j,sent in enumerate(sents):\n","      with open(f\"/content/test/{test_lang}/{emotions[i]}/{j}.txt\", 'w') as f: f.write(sent)"],"metadata":{"id":"QKe61lvJSfdR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-29T12:30:11.870942Z","iopub.status.busy":"2022-03-29T12:30:11.870743Z","iopub.status.idle":"2022-03-29T12:30:15.747979Z","shell.execute_reply":"2022-03-29T12:30:15.747350Z"},"id":"6IwI_2bcIeX8","executionInfo":{"status":"ok","timestamp":1669883002549,"user_tz":360,"elapsed":492,"user":{"displayName":"Isaac Schifferer","userId":"17126994690334067036"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ec0b3a74-c3a6-4c11-d748-efa77a7db2c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2357 files belonging to 5 classes.\n","Found 953 files belonging to 5 classes.\n"]}],"source":["# Convert to tf datasets\n","AUTOTUNE = tf.data.AUTOTUNE\n","batch_size = 1\n","seed = 42\n","\n","raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n","    f'/content/train/{train_lang}',\n","    batch_size=batch_size,\n","    seed=seed,label_mode='categorical')\n","train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-29T12:30:33.611866Z","iopub.status.busy":"2022-03-29T12:30:33.611329Z","iopub.status.idle":"2022-03-29T12:30:33.621515Z","shell.execute_reply":"2022-03-29T12:30:33.620990Z"},"id":"OWPOZE-L3AgE"},"outputs":[],"source":["loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n","metrics = [tf.metrics.CategoricalAccuracy(), \n","           tfa.metrics.F1Score(num_classes=len(emotions),average='weighted')]\n","\n","epochs = 12\n","steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n","num_train_steps = steps_per_epoch * epochs\n","num_warmup_steps = int(0.1*num_train_steps)\n","\n","init_lr = 1e-5\n","optimizer = optimization.create_optimizer(init_lr=init_lr,\n","                                          num_train_steps=num_train_steps,\n","                                          num_warmup_steps=num_warmup_steps,\n","                                          optimizer_type='adamw')"]},{"cell_type":"code","source":["# Load trained model\n","classifier_model = tf.keras.models.load_model(f\"/content/drive/MyDrive/ling490/TermProject/checkpoints/en12\")\n","classifier_model.compile(optimizer, loss, metrics)"],"metadata":{"id":"YJABBhVr0Ag3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evlauate model on each test language\n","for test_lang in [\"en\",\"fr\",\"pt\",\"zh\",\"id\",\"vi\",\"th\",\"bn\",\"de\",\"my\",\"tl\",\"km\"]:\n","  test_ds = tf.keras.utils.text_dataset_from_directory(\n","    f'/content/test/{test_lang}',\n","    batch_size=batch_size,label_mode='categorical')\n","  test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","\n","  loss_m, accuracy, f1 = classifier_model.evaluate(test_ds)\n","  print(test_lang)\n","  print(f'Loss: {loss_m}')\n","  print(f'Accuracy: {accuracy}')\n","  print(f'F1: {f1}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YJjzC6eGtISN","executionInfo":{"status":"ok","timestamp":1669883422379,"user_tz":360,"elapsed":392281,"user":{"displayName":"Isaac Schifferer","userId":"17126994690334067036"}},"outputId":"6ac680a2-1a06-42eb-f540-b06bb012885a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 590 files belonging to 5 classes.\n","590/590 [==============================] - 27s 36ms/step - loss: 2.4969 - categorical_accuracy: 0.6017 - f1_score: 0.5992\n","en\n","Loss: 2.4969141483306885\n","Accuracy: 0.6016949415206909\n","F1: 0.5992365479469299\n","Found 1314 files belonging to 5 classes.\n","1314/1314 [==============================] - 46s 35ms/step - loss: 2.6729 - categorical_accuracy: 0.5616 - f1_score: 0.5563\n","fr\n","Loss: 2.6729252338409424\n","Accuracy: 0.5616438388824463\n","F1: 0.5563379526138306\n","Found 592 files belonging to 5 classes.\n","592/592 [==============================] - 21s 36ms/step - loss: 2.6304 - categorical_accuracy: 0.5845 - f1_score: 0.5779\n","pt\n","Loss: 2.6303935050964355\n","Accuracy: 0.5844594836235046\n","F1: 0.5779013633728027\n","Found 592 files belonging to 5 classes.\n","592/592 [==============================] - 25s 43ms/step - loss: 2.0477 - categorical_accuracy: 0.6689 - f1_score: 0.6397\n","zh\n","Loss: 2.0476770401000977\n","Accuracy: 0.6689189076423645\n","F1: 0.6397345662117004\n","Found 1243 files belonging to 5 classes.\n","1243/1243 [==============================] - 47s 38ms/step - loss: 2.6770 - categorical_accuracy: 0.5615 - f1_score: 0.5484\n","id\n","Loss: 2.6769630908966064\n","Accuracy: 0.56154465675354\n","F1: 0.5483831763267517\n","Found 794 files belonging to 5 classes.\n","794/794 [==============================] - 31s 39ms/step - loss: 3.2411 - categorical_accuracy: 0.4824 - f1_score: 0.4621\n","vi\n","Loss: 3.2410783767700195\n","Accuracy: 0.48236775398254395\n","F1: 0.46206215023994446\n","Found 763 files belonging to 5 classes.\n","763/763 [==============================] - 26s 34ms/step - loss: 2.6062 - categorical_accuracy: 0.5780 - f1_score: 0.5500\n","th\n","Loss: 2.606219530105591\n","Accuracy: 0.5779816508293152\n","F1: 0.5500348210334778\n","Found 869 files belonging to 5 classes.\n","869/869 [==============================] - 35s 40ms/step - loss: 3.4861 - categorical_accuracy: 0.4534 - f1_score: 0.4284\n","bn\n","Loss: 3.4861316680908203\n","Accuracy: 0.45339471101760864\n","F1: 0.42842793464660645\n","Found 1182 files belonging to 5 classes.\n","1182/1182 [==============================] - 38s 32ms/step - loss: 2.7562 - categorical_accuracy: 0.5516 - f1_score: 0.5515\n","de\n","Loss: 2.7561850547790527\n","Accuracy: 0.5516074299812317\n","F1: 0.5515438318252563\n","Found 953 files belonging to 5 classes.\n","953/953 [==============================] - 38s 40ms/step - loss: 3.2348 - categorical_accuracy: 0.5173 - f1_score: 0.4980\n","my\n","Loss: 3.2347757816314697\n","Accuracy: 0.5173137187957764\n","F1: 0.4980388283729553\n","Found 590 files belonging to 5 classes.\n","590/590 [==============================] - 23s 38ms/step - loss: 3.1389 - categorical_accuracy: 0.5068 - f1_score: 0.4974\n","tl\n","Loss: 3.138934373855591\n","Accuracy: 0.506779670715332\n","F1: 0.4973539412021637\n","Found 977 files belonging to 5 classes.\n","977/977 [==============================] - 35s 35ms/step - loss: 3.3311 - categorical_accuracy: 0.5107 - f1_score: 0.4904\n","km\n","Loss: 3.3311173915863037\n","Accuracy: 0.5107471942901611\n","F1: 0.4904120862483978\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"accelerator":"GPU","gpuClass":"premium"},"nbformat":4,"nbformat_minor":0}